{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_External_Rohit_Plant_Classifier_IATL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3B7_FdLEs7Y",
        "colab_type": "text"
      },
      "source": [
        "### Plant Classifier Algorithm ###\n",
        "**Declaration for all libraries**\n",
        "*Important to run R7_External_Rohit_Plant_Classifier_Data_Load. *\n",
        "*Above script loads data from competition to the COLAB machine*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnZcvD2RvOwn",
        "colab_type": "code",
        "outputId": "5cfa0133-580f-4204-f7e4-5bf92c95bce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np # MATRIX OPERATIONS\n",
        "import pandas as pd # EFFICIENT DATA STRUCTURES\n",
        "import matplotlib.pyplot as plt # GRAPHING AND VISUALIZATIONS\n",
        "import math # MATHEMATICAL OPERATIONS\n",
        "import cv2 # IMAGE PROCESSING - OPENCV\n",
        "from glob import glob # FILE OPERATIONS\n",
        "import itertools\n",
        "\n",
        "# KERAS AND SKLEARN MODULES\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,CSVLogger\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbTQCzkcCFRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale = 256\n",
        "seed = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRTGCjH5FF07",
        "colab_type": "text"
      },
      "source": [
        "### 1. Read the images and generate the train and test dataset (5 points)\n",
        "### 2. Divide the data set into Train and validation data sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v8gmhP-DMSq",
        "colab_type": "text"
      },
      "source": [
        "* Step 1 is to read the data from the Folders\n",
        "* We will use Image Augmentation\n",
        "* Step 2 We will use sub classes of Image augmentation to split data into Training and Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6xWdFen7jAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mra40Ee6AAKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = ImageDataGenerator(validation_split=0.2, \n",
        "                          rescale=1/255,\n",
        "                          samplewise_center=False, # set input mean to 0 over the sample\n",
        "                          samplewise_std_normalization=False,  # divide inputs by std of the sample\n",
        "                          rotation_range=90,       # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                          width_shift_range=0.2,   # randomly shift images horizontally (fraction of total width)\n",
        "                          height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "                          fill_mode='reflect',     # filling the area outside\n",
        "                          zoom_range=0.4,          # random zoom\n",
        "                          horizontal_flip=True,    # randomly flip images\n",
        "                          vertical_flip=True)      # randomly flip images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA7bdsatAFGF",
        "colab_type": "code",
        "outputId": "f0e455ea-c4c6-488e-cdb8-28b794617a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = data.flow_from_directory(directory=\"train/\", \n",
        "                                           target_size=(scale,scale),\n",
        "                                           class_mode=\"categorical\", \n",
        "                                           shuffle=True, \n",
        "                                           batch_size=8, \n",
        "                                           subset='training')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3803 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qewhQHxJAfhY",
        "colab_type": "code",
        "outputId": "ec7c654c-7d40-456d-a085-ea1434f60d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_generator = data.flow_from_directory(directory=\"train/\", \n",
        "                                           target_size=(scale,scale),\n",
        "                                           class_mode=\"categorical\", \n",
        "                                           shuffle=True, \n",
        "                                           batch_size=8, \n",
        "                                           subset='validation')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 947 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuSyRcRxCiLC",
        "colab_type": "text"
      },
      "source": [
        "### 3. Initialize & build the model (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jvFtvJKDr3N",
        "colab_type": "text"
      },
      "source": [
        "* Step 1 We will use Transfer learning to use pre built models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixqlQEsJDpA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import ResNet50\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO92CbXLD5un",
        "colab_type": "code",
        "outputId": "072de23c-e938-43f1-f734-781b97bd0f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "conv_base = ResNet50(include_top=False, weights='imagenet', input_shape=(256,256,3), pooling=\"avg\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhPkn-FGEU2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in conv_base.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVlDwthX9ObD",
        "colab_type": "text"
      },
      "source": [
        "* Step 2 Optimize the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bOpF0oPEaM-",
        "colab_type": "code",
        "outputId": "ff5a46ef-b93a-4532-84cd-86db218981dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "x = conv_base.output\n",
        "x = Dense(512, activation='relu')(x) # let's add a fully-connected layer\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(128, activation='relu')(x) # let's add a fully-connected layer\n",
        "x = Dropout(0.25)(x)\n",
        "predictions = Dense(12, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "m = Model(inputs=conv_base.input, outputs=predictions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jDBckUCElOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oFItJJm9ecf",
        "colab_type": "text"
      },
      "source": [
        "### 5. Predict the accuracy for both train and validation data (7 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDWdS6YTEqnM",
        "colab_type": "code",
        "outputId": "8608466f-b35f-402d-bd36-858e5fd78f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "m_output =  m.fit_generator(train_generator, epochs=8, steps_per_epoch=1000 // 50,\n",
        "                            validation_data=val_generator, validation_steps=600 // 50,\n",
        "                            verbose = 1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/8\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.3036 - acc: 0.9161 - val_loss: 0.3135 - val_acc: 0.9167\n",
            "Epoch 2/8\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.3115 - acc: 0.9167 - val_loss: 0.2947 - val_acc: 0.9167\n",
            "Epoch 3/8\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.3021 - acc: 0.9167 - val_loss: 0.3080 - val_acc: 0.9167\n",
            "Epoch 4/8\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.2909 - acc: 0.9167 - val_loss: 0.2964 - val_acc: 0.9167\n",
            "Epoch 5/8\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.2954 - acc: 0.9167 - val_loss: 0.2974 - val_acc: 0.9167\n",
            "Epoch 6/8\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.2970 - acc: 0.9167 - val_loss: 0.2965 - val_acc: 0.9167\n",
            "Epoch 7/8\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.2909 - acc: 0.9167 - val_loss: 0.3057 - val_acc: 0.9167\n",
            "Epoch 8/8\n",
            "20/20 [==============================] - 94s 5s/step - loss: 0.2948 - acc: 0.9167 - val_loss: 0.3106 - val_acc: 0.9167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOLtfklmJibl",
        "colab_type": "text"
      },
      "source": [
        "*Step3 to Check the predictions of test folder*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDr3y4tFEvl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_test = '/test'\n",
        "pics = glob(path_to_test)\n",
        "\n",
        "testimages = []\n",
        "tests = []\n",
        "count=1\n",
        "num = len(pics)\n",
        "\n",
        "for i in pics:\n",
        "    print(str(count)+'/'+str(num),end='\\r')\n",
        "    tests.append(i.split('/')[-1])\n",
        "    testimages.append(cv2.resize(cv2.imread(i),(scale,scale)))\n",
        "    count = count + 1\n",
        "\n",
        "testimages = np.asarray(testimages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_GkkW3J30A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}